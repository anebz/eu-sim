{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from flair.data import Sentence\n",
    "from flair.embeddings import DocumentPoolEmbeddings, FlairEmbeddings, BertEmbeddings\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read file and extract sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_eu = \"goldstandard_eu_lexicover.tsv\"\n",
    "test_en = \"goldstandard_en_lexicover.tsv\"\n",
    "\n",
    "def get_gold_sentences(filename):\n",
    "    gold_sentences = []\n",
    "    with open(filename, 'rt') as f_p:\n",
    "        for line in f_p:\n",
    "            if line.startswith('\"origin\"'): # header\n",
    "                continue\n",
    "            \n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            line = line.rstrip()\n",
    "            line = line.replace('\"', '')\n",
    "            splitted = line.split('\\t')\n",
    "            gold = splitted[0]\n",
    "            sim_sentences = splitted[1:11]\n",
    "            \n",
    "            if gold:\n",
    "                gold_sentences_simple = {}\n",
    "                gold_sentences_simple[gold] = sim_sentences\n",
    "                gold_sentences.append(gold_sentences_simple)\n",
    "            \n",
    "    return gold_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent_eu = get_gold_sentences(test_eu)\n",
    "sent_en = get_gold_sentences(test_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def initialize_vectors(sent):\n",
    "    similarities_all = []\n",
    "    for i in range(len(sent)):\n",
    "        similarities_all.append([])\n",
    "\n",
    "    scores_all = []\n",
    "    for i in range(len(sent)):\n",
    "        scores_all.append([])\n",
    "        \n",
    "    return similarities_all, scores_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "similarities_all_eu, scores_all_eu = initialize_vectors(sent_eu)\n",
    "similarities_all_en, scores_all_en = initialize_vectors(sent_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load flair, BERT cased, BERT uncased embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# [[flair_eu, bert_cased_eu, bert_uncased eu], [flair_en, bert_cased_en, bert_uncased en]]\n",
    "embeddings_all = [[], []]\n",
    "embedding_names = [\"flair\", \"bert_cased\", \"bert_uncased\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load flair embeddings\n",
    "embeddings_all[0].append(DocumentPoolEmbeddings([FlairEmbeddings('eu-forward'), FlairEmbeddings('eu-backward')]))\n",
    "embeddings_all[1].append(DocumentPoolEmbeddings([FlairEmbeddings('mix-forward'), FlairEmbeddings('mix-backward')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-05 16:59:59,104 The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n",
      "2019-07-05 17:00:55,218 The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    }
   ],
   "source": [
    "# load BERT embeddings\n",
    "\n",
    "# See BERT paper, section 5.3 and table 7\n",
    "bert_layers = '-1,-2,-3,-4'\n",
    "bert_type = 'base' # 'large'\n",
    "\n",
    "# BERT cased\n",
    "embeddings_all[0].append(DocumentPoolEmbeddings([BertEmbeddings('bert-base-multilingual-cased', layers=bert_layers)]))\n",
    "embeddings_all[1].append(DocumentPoolEmbeddings([BertEmbeddings('bert-'+bert_type+'-cased', layers=bert_layers)]))\n",
    "\n",
    "# BERT uncased\n",
    "embeddings_all[0].append(DocumentPoolEmbeddings([BertEmbeddings('bert-base-multilingual-uncased', layers=bert_layers)]))\n",
    "embeddings_all[1].append(DocumentPoolEmbeddings([BertEmbeddings('bert-'+bert_type+'-uncased', layers=bert_layers)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculate similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_similarities(gold, sim_sentences, embeddings):\n",
    "    \n",
    "    similarities = []\n",
    "    query = gold\n",
    "\n",
    "    q = Sentence(query)\n",
    "    embeddings.embed(q)\n",
    "    score = 0\n",
    "    \n",
    "    for i in range(len(sim_sentences)):\n",
    "        \n",
    "        s = Sentence(sim_sentences[i])\n",
    "        embeddings.embed(s)\n",
    "\n",
    "        assert q.embedding.shape == s.embedding.shape\n",
    "        \n",
    "        cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "        prox = cos(q.embedding, s.embedding)\n",
    "        similarities.append(round(prox.item(), 4))\n",
    "        \n",
    "        if i > 0 and similarities[i] <= similarities[i-1]:\n",
    "            score += 1\n",
    "        \n",
    "    return similarities, score\n",
    "\n",
    "def calculate(gold_sentences, embeddings, similarities_all, scores_all):\n",
    "    for i in range(len(gold_sentences)):\n",
    "        \n",
    "        # obtain gold sentence and similar sentences from global list\n",
    "        gold = list(gold_sentences[i].keys())[0]\n",
    "        sim_sentences = gold_sentences[i][gold]\n",
    "        \n",
    "        # Calculate similarities for each 'gold' sentence and accumulated score\n",
    "        similarities, score = calculate_similarities(gold, sim_sentences, embeddings)\n",
    "\n",
    "        # append current similarity values and score to the global data structures\n",
    "        scores_all[i].append(score)\n",
    "        similarities_all[i].append(similarities)\n",
    "        \n",
    "    return similarities_all, scores_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating flair embeddings for basque\n",
      "Calculating flair embeddings for english\n",
      "Calculating bert_cased embeddings for basque\n",
      "Calculating bert_cased embeddings for english\n",
      "Calculating bert_uncased embeddings for basque\n",
      "Calculating bert_uncased embeddings for english\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(embeddings_all[0])):\n",
    "    print(f\"Calculating {embedding_names[i]} embeddings for basque\")\n",
    "    similarities_all_eu, scores_all_eu = calculate(sent_eu, embeddings_all[0][i], similarities_all_eu, scores_all_eu)\n",
    "    print(f\"Calculating {embedding_names[i]} embeddings for english\")\n",
    "    similarities_all_en, scores_all_en = calculate(sent_en, embeddings_all[1][i], similarities_all_en, scores_all_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Plot similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the format of your plot grid:\n",
      "[ (1,1) x1,y1 ]     [ (1,2) x2,y2 ]     [ (1,3) x3,y3 ]     [ (1,4) x4,y4 ]     [ (1,5) x5,y5 ]     [ (1,6) x6,y6 ]     [ (1,7) x7,y7 ]     [ (1,8) x8,y8 ]     [ (1,9) x9,y9 ]     [ (1,10) x10,y10 ]\n",
      "\n",
      "Sentence #0\n",
      "Basque\n",
      "Medikuak gaixoa bazkari batera gonbidatu zuen\n",
      "\n",
      "0. Medikuak gaixoa afari batera gonbidatu zuen\n",
      "\t flair similarity: 0.9597\n",
      "\t bert_cased similarity: 0.9842\n",
      "\t bert_uncased similarity: 0.9823\n",
      "1. Medikuak gaixoa bazkari batera gonbidatu du\n",
      "\t flair similarity: 0.9283\n",
      "\t bert_cased similarity: 0.9879\n",
      "\t bert_uncased similarity: 0.978\n",
      "2. Medikuak gaixoari esan zion, sendatuko zela\n",
      "\t flair similarity: 0.6528\n",
      "\t bert_cased similarity: 0.9237\n",
      "\t bert_uncased similarity: 0.9265\n",
      "3. Irakasleak ikaslea bazkari batera gonbidatu zuen\n",
      "\t flair similarity: 0.8803\n",
      "\t bert_cased similarity: 0.9593\n",
      "\t bert_uncased similarity: 0.9406\n",
      "4. Suhiltzaileak zientzialaria bazkari batera gonbidatu zuen\n",
      "\t flair similarity: 0.8831\n",
      "\t bert_cased similarity: 0.9216\n",
      "\t bert_uncased similarity: 0.9571\n",
      "5. Medikuak ez zuen gaixoa bazkari batera gonbidatu\n",
      "\t flair similarity: 0.7409\n",
      "\t bert_cased similarity: 0.9599\n",
      "\t bert_uncased similarity: 0.9711\n",
      "6. Gaixoak medikua bazkari batera gonbidatu zuen\n",
      "\t flair similarity: 0.9736\n",
      "\t bert_cased similarity: 0.9599\n",
      "\t bert_uncased similarity: 0.9908\n",
      "7. Zuen gaixoa batera bazkari medikuak gonbidatu\n",
      "\t flair similarity: 0.7472\n",
      "\t bert_cased similarity: 0.9452\n",
      "\t bert_uncased similarity: 0.9738\n",
      "8. Midekauk goaxia bakrazi betraa gibondtau zeun\n",
      "\t flair similarity: 0.5672\n",
      "\t bert_cased similarity: 0.8261\n",
      "\t bert_uncased similarity: 0.7873\n",
      "9. Utzi iezadazu laguntzen maletarekin\n",
      "\t flair similarity: 0.4903\n",
      "\t bert_cased similarity: 0.8404\n",
      "\t bert_uncased similarity: 0.8676\n",
      "Scores: flair: 6, bert_cased: 5, bert_uncased: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "colorscale": "Blues",
         "type": "heatmap",
         "uid": "f508a8a7-2633-47b8-94fe-7193afa3f6b8",
         "y": [
          "flair",
          "bert_cased",
          "bert_uncased"
         ],
         "z": [
          [
           0.9597,
           0.9283,
           0.6528,
           0.8803,
           0.8831,
           0.7409,
           0.9736,
           0.7472,
           0.5672,
           0.4903
          ],
          [
           0.9842,
           0.9879,
           0.9237,
           0.9593,
           0.9216,
           0.9599,
           0.9599,
           0.9452,
           0.8261,
           0.8404
          ],
          [
           0.9823,
           0.978,
           0.9265,
           0.9406,
           0.9571,
           0.9711,
           0.9908,
           0.9738,
           0.7873,
           0.8676
          ]
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"b73f0c44-1669-462f-bd0e-2fc5b56c7ae7\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    window.PLOTLYENV.BASE_URL='https://plot.ly';\n",
       "                    \n",
       "                if (document.getElementById(\"b73f0c44-1669-462f-bd0e-2fc5b56c7ae7\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        'b73f0c44-1669-462f-bd0e-2fc5b56c7ae7',\n",
       "                        [{\"colorscale\": \"Blues\", \"type\": \"heatmap\", \"uid\": \"f508a8a7-2633-47b8-94fe-7193afa3f6b8\", \"y\": [\"flair\", \"bert_cased\", \"bert_uncased\"], \"z\": [[0.9597, 0.9283, 0.6528, 0.8803, 0.8831, 0.7409, 0.9736, 0.7472, 0.5672, 0.4903], [0.9842, 0.9879, 0.9237, 0.9593, 0.9216, 0.9599, 0.9599, 0.9452, 0.8261, 0.8404], [0.9823, 0.978, 0.9265, 0.9406, 0.9571, 0.9711, 0.9908, 0.9738, 0.7873, 0.8676]]}],\n",
       "                        {},\n",
       "                        {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('b73f0c44-1669-462f-bd0e-2fc5b56c7ae7');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English\n",
      "the doctor invited the patient for lunch\n",
      "\n",
      "0. the doctor invited the patient for dinner\n",
      "\t flair similarity: 0.979\n",
      "\t bert_cased similarity: 0.9943\n",
      "\t bert_uncased similarity: 0.9781\n",
      "1. the doctor has invited the patient for lunch\n",
      "\t flair similarity: 0.9799\n",
      "\t bert_cased similarity: 0.9846\n",
      "\t bert_uncased similarity: 0.9366\n",
      "2. the doctor told the patient, they would get better\n",
      "\t flair similarity: 0.7886\n",
      "\t bert_cased similarity: 0.9407\n",
      "\t bert_uncased similarity: 0.8225\n",
      "3. the teacher invited the student for lunch\n",
      "\t flair similarity: 0.9567\n",
      "\t bert_cased similarity: 0.9468\n",
      "\t bert_uncased similarity: 0.8718\n",
      "4. the firefighter invited the scientist for lunch\n",
      "\t flair similarity: 0.9466\n",
      "\t bert_cased similarity: 0.973\n",
      "\t bert_uncased similarity: 0.9007\n",
      "5. the patient did not invite the doctor for lunch\n",
      "\t flair similarity: 0.9127\n",
      "\t bert_cased similarity: 0.9667\n",
      "\t bert_uncased similarity: 0.8959\n",
      "6. the patient invited the doctor for lunch\n",
      "\t flair similarity: 0.9852\n",
      "\t bert_cased similarity: 0.9965\n",
      "\t bert_uncased similarity: 0.984\n",
      "7. patient invited the lunch the for doctor\n",
      "\t flair similarity: 0.9022\n",
      "\t bert_cased similarity: 0.9452\n",
      "\t bert_uncased similarity: 0.7127\n",
      "8. eth roctod ivtedni hte pietant fro chunl\n",
      "\t flair similarity: 0.4821\n",
      "\t bert_cased similarity: 0.773\n",
      "\t bert_uncased similarity: 0.4075\n",
      "9. let me help you with that suitcase\n",
      "\t flair similarity: 0.6245\n",
      "\t bert_cased similarity: 0.8512\n",
      "\t bert_uncased similarity: 0.4863\n",
      "Scores: flair: 5, bert_cased: 5, bert_uncased: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "colorscale": "Blues",
         "type": "heatmap",
         "uid": "a221523f-9bb9-47ec-ae3c-b3cf77d88570",
         "y": [
          "flair",
          "bert_cased",
          "bert_uncased"
         ],
         "z": [
          [
           0.979,
           0.9799,
           0.7886,
           0.9567,
           0.9466,
           0.9127,
           0.9852,
           0.9022,
           0.4821,
           0.6245
          ],
          [
           0.9943,
           0.9846,
           0.9407,
           0.9468,
           0.973,
           0.9667,
           0.9965,
           0.9452,
           0.773,
           0.8512
          ],
          [
           0.9781,
           0.9366,
           0.8225,
           0.8718,
           0.9007,
           0.8959,
           0.984,
           0.7127,
           0.4075,
           0.4863
          ]
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"1d99b3bd-214e-4772-ada4-3e57caf1f384\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    window.PLOTLYENV.BASE_URL='https://plot.ly';\n",
       "                    \n",
       "                if (document.getElementById(\"1d99b3bd-214e-4772-ada4-3e57caf1f384\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '1d99b3bd-214e-4772-ada4-3e57caf1f384',\n",
       "                        [{\"colorscale\": \"Blues\", \"type\": \"heatmap\", \"uid\": \"a221523f-9bb9-47ec-ae3c-b3cf77d88570\", \"y\": [\"flair\", \"bert_cased\", \"bert_uncased\"], \"z\": [[0.979, 0.9799, 0.7886, 0.9567, 0.9466, 0.9127, 0.9852, 0.9022, 0.4821, 0.6245], [0.9943, 0.9846, 0.9407, 0.9468, 0.973, 0.9667, 0.9965, 0.9452, 0.773, 0.8512], [0.9781, 0.9366, 0.8225, 0.8718, 0.9007, 0.8959, 0.984, 0.7127, 0.4075, 0.4863]]}],\n",
       "                        {},\n",
       "                        {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('1d99b3bd-214e-4772-ada4-3e57caf1f384');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plotly.tools.make_subplots(rows=1, cols=10)\n",
    "\n",
    "def plot_similarities(sent, similarities_all, scores_all):\n",
    "    origin = list(sent[i].keys())[0]\n",
    "    # print origin sentence\n",
    "    print(origin + '\\n')\n",
    "    for j in range(len(sent[i][origin])):\n",
    "        # print each similar sentence\n",
    "        print(f\"{j}. {sent[i][origin][j]}\")\n",
    "        for k in range(len(embedding_names)):\n",
    "            # print each similarity value for each variant\n",
    "            print(f\"\\t {embedding_names[k]} similarity: {similarities_all[i][k][j]}\")\n",
    "    # print scores for all variants\n",
    "    print(f\"Scores: \" + \", \".join(f\"{embed}: {scor}\" for embed, scor in zip(embedding_names, scores_all[i])))\n",
    "    \n",
    "    # plot similarity heatmap\n",
    "    trace = go.Heatmap(z=similarities_all[i], y=embedding_names, colorscale='Blues')\n",
    "    data=[trace]\n",
    "    fig.append_trace(trace, 1, i+1)\n",
    "    iplot(data, filename='basic-heatmap' + str(i))\n",
    "\n",
    "for i in range(len(sent_eu)):\n",
    "    \n",
    "    print(f\"Sentence #{i}\")\n",
    "    print(\"Basque\")\n",
    "    plot_similarities(sent_eu, similarities_all_eu, scores_all_eu)\n",
    "    print(\"English\")\n",
    "    plot_similarities(sent_en, similarities_all_en, scores_all_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculate total scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def calculate_total_score(scores_all):\n",
    "    total_scores = [0] * len(scores_all[0])\n",
    "    for i in range(len(scores_all)):\n",
    "        total_scores[0] += scores_all[i][0]\n",
    "        total_scores[1] += scores_all[i][1]\n",
    "        total_scores[2] += scores_all[i][2]\n",
    "    print(f\"Total scores: \" + \", \".join(f\"{embed}: {scor}\" for embed, scor in zip(embedding_names, total_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total scores: flair: 6, bert_cased: 5, bert_uncased: 4\n",
      "Total scores: flair: 5, bert_cased: 5, bert_uncased: 5\n"
     ]
    }
   ],
   "source": [
    "calculate_total_score(scores_all_eu)\n",
    "calculate_total_score(scores_all_en)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
