{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from flair.data import Sentence\n",
    "from flair.embeddings import DocumentPoolEmbeddings, FlairEmbeddings, BertEmbeddings, ELMoEmbeddings\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# [[flair_eu, bert_cased_eu, bert_uncased eu, elmo_eu], [flair_en, bert_cased_en, bert_uncased en, elmo_en]]\n",
    "embeddings_all = [[], []]\n",
    "embedding_names = [\"flair\", \"bert_cased\", \"bert_uncased\", \"elmo\"]\n",
    "#embedding_names = [\"bert4c\", \"bert4u\", \"bert3c\", \"bert3u\", \"bert2c\",\"bert2u\", \"bert1c\", \"bert1u\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load flair embeddings\n",
    "embeddings_all[0].append(DocumentPoolEmbeddings([FlairEmbeddings('eu-forward'), FlairEmbeddings('eu-backward')]))\n",
    "embeddings_all[1].append(DocumentPoolEmbeddings([FlairEmbeddings('mix-forward'), FlairEmbeddings('mix-backward')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-17 18:37:31,777 The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n",
      "2019-07-17 18:37:57,467 The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n",
      "2019-07-17 18:38:52,903 The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n",
      "2019-07-17 18:39:20,593 The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n",
      "2019-07-17 18:40:36,884 The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n",
      "2019-07-17 18:41:14,547 The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n",
      "2019-07-17 18:42:23,837 The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n",
      "2019-07-17 18:42:58,888 The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    }
   ],
   "source": [
    "# load BERT embeddings\n",
    "\n",
    "# See BERT paper, section 5.3 and table 7\n",
    "#bert_layers = list(range(-9, -13, -1))\n",
    "bert_layers = '-1,-2,-3,-4'\n",
    "bert_type = 'base' # 'large'\n",
    "\n",
    "# BERT cased\n",
    "embeddings_all[0].append(DocumentPoolEmbeddings([BertEmbeddings('bert-base-multilingual-cased', layers=bert_layers)]))\n",
    "embeddings_all[1].append(DocumentPoolEmbeddings([BertEmbeddings('bert-'+bert_type+'-cased', layers=bert_layers)]))\n",
    "\n",
    "# BERT uncased\n",
    "embeddings_all[0].append(DocumentPoolEmbeddings([BertEmbeddings('bert-base-multilingual-uncased', layers=bert_layers)]))\n",
    "embeddings_all[1].append(DocumentPoolEmbeddings([BertEmbeddings('bert-'+bert_type+'-uncased', layers=bert_layers)]))\n",
    "\n",
    "# bert_layer case\n",
    "'''\n",
    "\n",
    "for layer in bert_layers:\n",
    "\n",
    "    # BERT cased\n",
    "    embeddings_all[0].append(DocumentPoolEmbeddings([BertEmbeddings('bert-base-multilingual-cased', layers=str(layer))]))\n",
    "    embeddings_all[1].append(DocumentPoolEmbeddings([BertEmbeddings('bert-'+bert_type+'-cased', layers=str(layer))]))\n",
    "\n",
    "    # BERT uncased\n",
    "    embeddings_all[0].append(DocumentPoolEmbeddings([BertEmbeddings('bert-base-multilingual-uncased', layers=str(layer))]))\n",
    "    embeddings_all[1].append(DocumentPoolEmbeddings([BertEmbeddings('bert-'+bert_type+'-uncased', layers=str(layer))]))\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ELMo embeddings\n",
    "embeddings_all[0].append(DocumentPoolEmbeddings([ELMoEmbeddings(options_file=\"https://schweter.eu/cloud/eu-elmo/options.json\", \n",
    "                                                                weight_file=\"https://schweter.eu/cloud/eu-elmo/weights.hdf5\")]))\n",
    "embeddings_all[1].append(DocumentPoolEmbeddings([ELMoEmbeddings()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read file and extract sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_gold_sentences(filename):\n",
    "    gold_sentences = []\n",
    "    with open(filename, 'rt') as f_p:\n",
    "        for line in f_p:\n",
    "            if line.startswith('\"origin\"'): # header\n",
    "                continue\n",
    "            \n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            line = line.rstrip()\n",
    "            line = line.replace('\"', '')\n",
    "            splitted = line.split('\\t')\n",
    "            gold = splitted[0]\n",
    "            sim_sentences = splitted[1:11]\n",
    "            \n",
    "            if gold:\n",
    "                gold_sentences_simple = {}\n",
    "                gold_sentences_simple[gold] = sim_sentences\n",
    "                gold_sentences.append(gold_sentences_simple)\n",
    "            \n",
    "    return gold_sentences\n",
    "\n",
    "def initialize_vectors(sent):\n",
    "    similarities_all = []\n",
    "    for i in range(len(sent)):\n",
    "        similarities_all.append([])\n",
    "\n",
    "    scores_all = []\n",
    "    for i in range(len(sent)):\n",
    "        scores_all.append([])\n",
    "        \n",
    "    return similarities_all, scores_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_eu = \"goldstandard_eu_lexicover.tsv\"\n",
    "test_en = \"goldstandard_en_lexicover.tsv\"\n",
    "\n",
    "sent_eu = get_gold_sentences(test_eu)\n",
    "sent_en = get_gold_sentences(test_en)\n",
    "\n",
    "similarities_all_eu, scores_all_eu = initialize_vectors(sent_eu)\n",
    "similarities_all_en, scores_all_en = initialize_vectors(sent_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculate similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_similarities(gold, sim_sentences, embeddings):\n",
    "    \n",
    "    similarities = []\n",
    "    query = gold\n",
    "\n",
    "    q = Sentence(query)\n",
    "    embeddings.embed(q)\n",
    "    \n",
    "    print(q)\n",
    "    print(q.embedding)\n",
    "    \n",
    "    score = 0\n",
    "    \n",
    "    for i in range(len(sim_sentences)):\n",
    "        \n",
    "        s = Sentence(sim_sentences[i])\n",
    "        embeddings.embed(s)\n",
    "\n",
    "        assert q.embedding.shape == s.embedding.shape\n",
    "        \n",
    "        cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "        prox = cos(q.embedding, s.embedding)\n",
    "        similarities.append(round(prox.item(), 4))\n",
    "        \n",
    "        if i > 0 and similarities[i] <= similarities[i-1]:\n",
    "            score += 1\n",
    "        \n",
    "    return similarities, int(score/float(len(sim_sentences))*100)\n",
    "\n",
    "def calculate(gold_sentences, embeddings, similarities_all, scores_all):\n",
    "    \n",
    "    for i in range(len(gold_sentences)):\n",
    "        \n",
    "        # obtain gold sentence and similar sentences from global list\n",
    "        gold = list(gold_sentences[i].keys())[0]\n",
    "        sim_sentences = gold_sentences[i][gold]\n",
    "        \n",
    "        # Calculate similarities for each 'gold' sentence and accumulated score\n",
    "        similarities, score = calculate_similarities(gold, sim_sentences, embeddings)\n",
    "\n",
    "        # append current similarity values and score to the global data structures\n",
    "        scores_all[i].append(score)\n",
    "        similarities_all[i].append(similarities)\n",
    "\n",
    "        \n",
    "    return similarities_all, scores_al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating flair embeddings for basque\n",
      "Sentence: \"medikuak gaixoa bazkaltzera gonbidatu zuen\" - 5 Tokens\n",
      "tensor([-0.0302,  0.3141,  0.0454,  ..., -0.0182,  0.0214,  0.1127],\n",
      "       grad_fn=<CatBackward>)\n",
      "Sentence: \"gaixoak medikua bazkaltzera gonbidatu zuen\" - 5 Tokens\n",
      "tensor([-0.0405,  0.3071,  0.0508,  ..., -0.0208,  0.0203,  0.0724],\n",
      "       grad_fn=<CatBackward>)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-c46cb939bb0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings_all\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Calculating {embedding_names[i]} embeddings for basque\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msimilarities_all_eu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores_all_eu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent_eu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings_all\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimilarities_all_eu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores_all_eu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Calculating {embedding_names[i]} embeddings for english\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0msimilarities_all_en\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores_all_en\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent_en\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings_all\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimilarities_all_en\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores_all_en\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-b596e7652974>\u001b[0m in \u001b[0;36mcalculate\u001b[1;34m(gold_sentences, embeddings, similarities_all, scores_all)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;31m# Calculate similarities for each 'gold' sentence and accumulated score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0msimilarities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_similarities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msim_sentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "for i in range(len(embeddings_all[0])):\n",
    "    print(f\"Calculating {embedding_names[i]} embeddings for basque\")\n",
    "    similarities_all_eu, scores_all_eu = calculate(sent_eu, embeddings_all[0][i], similarities_all_eu, scores_all_eu)\n",
    "    print(f\"Calculating {embedding_names[i]} embeddings for english\")\n",
    "    similarities_all_en, scores_all_en = calculate(sent_en, embeddings_all[1][i], similarities_all_en, scores_all_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Plot similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the format of your plot grid:\n",
      "[ (1,1) x1,y1 ]     [ (1,2) x2,y2 ]     [ (1,3) x3,y3 ]     [ (1,4) x4,y4 ]     [ (1,5) x5,y5 ]     [ (1,6) x6,y6 ]     [ (1,7) x7,y7 ]     [ (1,8) x8,y8 ]     [ (1,9) x9,y9 ]     [ (1,10) x10,y10 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fig = plotly.tools.make_subplots(rows=1, cols=10)\n",
    "\n",
    "def plot_similarities(sent, similarities_all, scores_all):\n",
    "    origin = list(sent[i].keys())[0]\n",
    "    # print origin sentence\n",
    "    print(origin + '\\n')\n",
    "    for j in range(len(sent[i][origin])):\n",
    "        # print each similar sentence\n",
    "        print(f\"{j}. {sent[i][origin][j]}\")\n",
    "        for k in range(len(embedding_names)):\n",
    "            # print each similarity value for each variant\n",
    "            print(f\"\\t {embedding_names[k]} similarity: {similarities_all[i][k][j]}\")\n",
    "    # print scores for all variants\n",
    "    print(f\"Scores: \" + \", \".join(f\"{embed}: {scor}%\" for embed, scor in zip(embedding_names, scores_all[i])))\n",
    "    \n",
    "    # plot similarity heatmap\n",
    "    trace = go.Heatmap(z=similarities_all[i], y=embedding_names, colorscale='Blues')\n",
    "    data=[trace]\n",
    "    fig.append_trace(trace, 1, i+1)\n",
    "\n",
    "    iplot(data, filename='basic-heatmap' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence #0\n",
      "English\n",
      "the doctor invited the patient for lunch\n",
      "\n",
      "0. the patient invited the doctor for lunch\n",
      "\t bert4c similarity: 0.9982\n",
      "\t bert4u similarity: 0.9962\n",
      "\t bert3c similarity: 0.9988\n",
      "\t bert3u similarity: 0.9977\n",
      "\t bert2c similarity: 0.9992\n",
      "\t bert2u similarity: 0.9987\n",
      "\t bert1c similarity: 0.9998\n",
      "\t bert1u similarity: 0.9989\n",
      "1. the lunch invited the doctor for the patient\n",
      "\t bert4c similarity: 0.984\n",
      "\t bert4u similarity: 0.9732\n",
      "\t bert3c similarity: 0.986\n",
      "\t bert3u similarity: 0.9821\n",
      "\t bert2c similarity: 0.9887\n",
      "\t bert2u similarity: 0.9882\n",
      "\t bert1c similarity: 0.9897\n",
      "\t bert1u similarity: 0.9877\n",
      "2. for invited patient the doctor the lunch\n",
      "\t bert4c similarity: 0.9607\n",
      "\t bert4u similarity: 0.9111\n",
      "\t bert3c similarity: 0.9765\n",
      "\t bert3u similarity: 0.9556\n",
      "\t bert2c similarity: 0.9907\n",
      "\t bert2u similarity: 0.9848\n",
      "\t bert1c similarity: 0.999\n",
      "\t bert1u similarity: 0.9946\n",
      "Scores: bert4c: 66%, bert4u: 66%, bert3c: 66%, bert3u: 66%, bert2c: 33%, bert2u: 66%, bert1c: 33%, bert1u: 33%\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "colorscale": "Blues",
         "type": "heatmap",
         "uid": "ccc588a2-fd45-44ef-8ba7-4b8e0156cd7a",
         "y": [
          "bert4c",
          "bert4u",
          "bert3c",
          "bert3u",
          "bert2c",
          "bert2u",
          "bert1c",
          "bert1u"
         ],
         "z": [
          [
           0.9982,
           0.984,
           0.9607
          ],
          [
           0.9962,
           0.9732,
           0.9111
          ],
          [
           0.9988,
           0.986,
           0.9765
          ],
          [
           0.9977,
           0.9821,
           0.9556
          ],
          [
           0.9992,
           0.9887,
           0.9907
          ],
          [
           0.9987,
           0.9882,
           0.9848
          ],
          [
           0.9998,
           0.9897,
           0.999
          ],
          [
           0.9989,
           0.9877,
           0.9946
          ]
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"a418f5ef-57e1-4ed3-8b7d-f8b2c472662b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    window.PLOTLYENV.BASE_URL='https://plot.ly';\n",
       "                    \n",
       "                if (document.getElementById(\"a418f5ef-57e1-4ed3-8b7d-f8b2c472662b\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        'a418f5ef-57e1-4ed3-8b7d-f8b2c472662b',\n",
       "                        [{\"colorscale\": \"Blues\", \"type\": \"heatmap\", \"uid\": \"ccc588a2-fd45-44ef-8ba7-4b8e0156cd7a\", \"y\": [\"bert4c\", \"bert4u\", \"bert3c\", \"bert3u\", \"bert2c\", \"bert2u\", \"bert1c\", \"bert1u\"], \"z\": [[0.9982, 0.984, 0.9607], [0.9962, 0.9732, 0.9111], [0.9988, 0.986, 0.9765], [0.9977, 0.9821, 0.9556], [0.9992, 0.9887, 0.9907], [0.9987, 0.9882, 0.9848], [0.9998, 0.9897, 0.999], [0.9989, 0.9877, 0.9946]]}],\n",
       "                        {},\n",
       "                        {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('a418f5ef-57e1-4ed3-8b7d-f8b2c472662b');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence #1\n",
      "English\n",
      "the doctor invited the patient for lunch\n",
      "\n",
      "0. the patient invited the doctor for lunch\n",
      "\t bert4c similarity: 0.9982\n",
      "\t bert4u similarity: 0.9962\n",
      "\t bert3c similarity: 0.9988\n",
      "\t bert3u similarity: 0.9977\n",
      "\t bert2c similarity: 0.9992\n",
      "\t bert2u similarity: 0.9987\n",
      "\t bert1c similarity: 0.9998\n",
      "\t bert1u similarity: 0.9989\n",
      "1. the doctor did not invite the patient for lunch\n",
      "\t bert4c similarity: 0.9567\n",
      "\t bert4u similarity: 0.9373\n",
      "\t bert3c similarity: 0.9485\n",
      "\t bert3u similarity: 0.9249\n",
      "\t bert2c similarity: 0.9436\n",
      "\t bert2u similarity: 0.9277\n",
      "\t bert1c similarity: 0.9323\n",
      "\t bert1u similarity: 0.9036\n",
      "2. the child invited the grandfather for lunch\n",
      "\t bert4c similarity: 0.9268\n",
      "\t bert4u similarity: 0.9024\n",
      "\t bert3c similarity: 0.8998\n",
      "\t bert3u similarity: 0.8626\n",
      "\t bert2c similarity: 0.8708\n",
      "\t bert2u similarity: 0.8421\n",
      "\t bert1c similarity: 0.8292\n",
      "\t bert1u similarity: 0.7853\n",
      "3. the doctor killed the patient after lunch\n",
      "\t bert4c similarity: 0.9343\n",
      "\t bert4u similarity: 0.9087\n",
      "\t bert3c similarity: 0.9173\n",
      "\t bert3u similarity: 0.9054\n",
      "\t bert2c similarity: 0.9088\n",
      "\t bert2u similarity: 0.8949\n",
      "\t bert1c similarity: 0.8812\n",
      "\t bert1u similarity: 0.8435\n",
      "4. that is a matter between the doctor and the patient\n",
      "\t bert4c similarity: 0.8669\n",
      "\t bert4u similarity: 0.8075\n",
      "\t bert3c similarity: 0.8291\n",
      "\t bert3u similarity: 0.7966\n",
      "\t bert2c similarity: 0.8164\n",
      "\t bert2u similarity: 0.7979\n",
      "\t bert1c similarity: 0.769\n",
      "\t bert1u similarity: 0.7307\n",
      "5. I wish I got invited for lunch\n",
      "\t bert4c similarity: 0.8396\n",
      "\t bert4u similarity: 0.7608\n",
      "\t bert3c similarity: 0.801\n",
      "\t bert3u similarity: 0.7299\n",
      "\t bert2c similarity: 0.7633\n",
      "\t bert2u similarity: 0.7323\n",
      "\t bert1c similarity: 0.7045\n",
      "\t bert1u similarity: 0.6718\n",
      "Scores: bert4c: 66%, bert4u: 66%, bert3c: 66%, bert3u: 66%, bert2c: 66%, bert2u: 66%, bert1c: 66%, bert1u: 66%\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "colorscale": "Blues",
         "type": "heatmap",
         "uid": "04b37a3f-e300-4a35-a66d-48a9be510de1",
         "y": [
          "bert4c",
          "bert4u",
          "bert3c",
          "bert3u",
          "bert2c",
          "bert2u",
          "bert1c",
          "bert1u"
         ],
         "z": [
          [
           0.9982,
           0.9567,
           0.9268,
           0.9343,
           0.8669,
           0.8396
          ],
          [
           0.9962,
           0.9373,
           0.9024,
           0.9087,
           0.8075,
           0.7608
          ],
          [
           0.9988,
           0.9485,
           0.8998,
           0.9173,
           0.8291,
           0.801
          ],
          [
           0.9977,
           0.9249,
           0.8626,
           0.9054,
           0.7966,
           0.7299
          ],
          [
           0.9992,
           0.9436,
           0.8708,
           0.9088,
           0.8164,
           0.7633
          ],
          [
           0.9987,
           0.9277,
           0.8421,
           0.8949,
           0.7979,
           0.7323
          ],
          [
           0.9998,
           0.9323,
           0.8292,
           0.8812,
           0.769,
           0.7045
          ],
          [
           0.9989,
           0.9036,
           0.7853,
           0.8435,
           0.7307,
           0.6718
          ]
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"70da3832-5318-41c8-8524-af863a1ac152\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    window.PLOTLYENV.BASE_URL='https://plot.ly';\n",
       "                    \n",
       "                if (document.getElementById(\"70da3832-5318-41c8-8524-af863a1ac152\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '70da3832-5318-41c8-8524-af863a1ac152',\n",
       "                        [{\"colorscale\": \"Blues\", \"type\": \"heatmap\", \"uid\": \"04b37a3f-e300-4a35-a66d-48a9be510de1\", \"y\": [\"bert4c\", \"bert4u\", \"bert3c\", \"bert3u\", \"bert2c\", \"bert2u\", \"bert1c\", \"bert1u\"], \"z\": [[0.9982, 0.9567, 0.9268, 0.9343, 0.8669, 0.8396], [0.9962, 0.9373, 0.9024, 0.9087, 0.8075, 0.7608], [0.9988, 0.9485, 0.8998, 0.9173, 0.8291, 0.801], [0.9977, 0.9249, 0.8626, 0.9054, 0.7966, 0.7299], [0.9992, 0.9436, 0.8708, 0.9088, 0.8164, 0.7633], [0.9987, 0.9277, 0.8421, 0.8949, 0.7979, 0.7323], [0.9998, 0.9323, 0.8292, 0.8812, 0.769, 0.7045], [0.9989, 0.9036, 0.7853, 0.8435, 0.7307, 0.6718]]}],\n",
       "                        {},\n",
       "                        {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('70da3832-5318-41c8-8524-af863a1ac152');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence #2\n",
      "English\n",
      "the doctor invited the patient for lunch\n",
      "\n",
      "0. the surgeon invited the patient for lunch\n",
      "\t bert4c similarity: 0.9842\n",
      "\t bert4u similarity: 0.9699\n",
      "\t bert3c similarity: 0.9777\n",
      "\t bert3u similarity: 0.9622\n",
      "\t bert2c similarity: 0.9706\n",
      "\t bert2u similarity: 0.9514\n",
      "\t bert1c similarity: 0.9566\n",
      "\t bert1u similarity: 0.9358\n",
      "1. the doctor invited the doctor for lunch\n",
      "\t bert4c similarity: 0.9747\n",
      "\t bert4u similarity: 0.9502\n",
      "\t bert3c similarity: 0.9662\n",
      "\t bert3u similarity: 0.9436\n",
      "\t bert2c similarity: 0.9619\n",
      "\t bert2u similarity: 0.9441\n",
      "\t bert1c similarity: 0.9511\n",
      "\t bert1u similarity: 0.9248\n",
      "2. the professor invited the patient for lunch\n",
      "\t bert4c similarity: 0.974\n",
      "\t bert4u similarity: 0.9627\n",
      "\t bert3c similarity: 0.9653\n",
      "\t bert3u similarity: 0.9519\n",
      "\t bert2c similarity: 0.9552\n",
      "\t bert2u similarity: 0.9442\n",
      "\t bert1c similarity: 0.9391\n",
      "\t bert1u similarity: 0.9289\n",
      "3. the doctor invited the patient for a meal\n",
      "\t bert4c similarity: 0.9742\n",
      "\t bert4u similarity: 0.9578\n",
      "\t bert3c similarity: 0.9635\n",
      "\t bert3u similarity: 0.9545\n",
      "\t bert2c similarity: 0.9612\n",
      "\t bert2u similarity: 0.9482\n",
      "\t bert1c similarity: 0.9468\n",
      "\t bert1u similarity: 0.9294\n",
      "4. the doctor took the patient out for tea\n",
      "\t bert4c similarity: 0.94\n",
      "\t bert4u similarity: 0.9056\n",
      "\t bert3c similarity: 0.9143\n",
      "\t bert3u similarity: 0.8891\n",
      "\t bert2c similarity: 0.8975\n",
      "\t bert2u similarity: 0.8725\n",
      "\t bert1c similarity: 0.8655\n",
      "\t bert1u similarity: 0.8376\n",
      "5. the doctor paid for the patient's lunch\n",
      "\t bert4c similarity: 0.9493\n",
      "\t bert4u similarity: 0.9161\n",
      "\t bert3c similarity: 0.9384\n",
      "\t bert3u similarity: 0.9177\n",
      "\t bert2c similarity: 0.9334\n",
      "\t bert2u similarity: 0.9141\n",
      "\t bert1c similarity: 0.9238\n",
      "\t bert1u similarity: 0.8821\n",
      "Scores: bert4c: 50%, bert4u: 50%, bert3c: 66%, bert3u: 33%, bert2c: 50%, bert2u: 33%, bert1c: 50%, bert1u: 33%\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "colorscale": "Blues",
         "type": "heatmap",
         "uid": "146a1f2a-9231-4e9f-9d5e-ce1fb87cca99",
         "y": [
          "bert4c",
          "bert4u",
          "bert3c",
          "bert3u",
          "bert2c",
          "bert2u",
          "bert1c",
          "bert1u"
         ],
         "z": [
          [
           0.9842,
           0.9747,
           0.974,
           0.9742,
           0.94,
           0.9493
          ],
          [
           0.9699,
           0.9502,
           0.9627,
           0.9578,
           0.9056,
           0.9161
          ],
          [
           0.9777,
           0.9662,
           0.9653,
           0.9635,
           0.9143,
           0.9384
          ],
          [
           0.9622,
           0.9436,
           0.9519,
           0.9545,
           0.8891,
           0.9177
          ],
          [
           0.9706,
           0.9619,
           0.9552,
           0.9612,
           0.8975,
           0.9334
          ],
          [
           0.9514,
           0.9441,
           0.9442,
           0.9482,
           0.8725,
           0.9141
          ],
          [
           0.9566,
           0.9511,
           0.9391,
           0.9468,
           0.8655,
           0.9238
          ],
          [
           0.9358,
           0.9248,
           0.9289,
           0.9294,
           0.8376,
           0.8821
          ]
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"ef74dd41-ac8d-4748-b2c3-8d981010bd1e\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    window.PLOTLYENV.BASE_URL='https://plot.ly';\n",
       "                    \n",
       "                if (document.getElementById(\"ef74dd41-ac8d-4748-b2c3-8d981010bd1e\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        'ef74dd41-ac8d-4748-b2c3-8d981010bd1e',\n",
       "                        [{\"colorscale\": \"Blues\", \"type\": \"heatmap\", \"uid\": \"146a1f2a-9231-4e9f-9d5e-ce1fb87cca99\", \"y\": [\"bert4c\", \"bert4u\", \"bert3c\", \"bert3u\", \"bert2c\", \"bert2u\", \"bert1c\", \"bert1u\"], \"z\": [[0.9842, 0.9747, 0.974, 0.9742, 0.94, 0.9493], [0.9699, 0.9502, 0.9627, 0.9578, 0.9056, 0.9161], [0.9777, 0.9662, 0.9653, 0.9635, 0.9143, 0.9384], [0.9622, 0.9436, 0.9519, 0.9545, 0.8891, 0.9177], [0.9706, 0.9619, 0.9552, 0.9612, 0.8975, 0.9334], [0.9514, 0.9441, 0.9442, 0.9482, 0.8725, 0.9141], [0.9566, 0.9511, 0.9391, 0.9468, 0.8655, 0.9238], [0.9358, 0.9248, 0.9289, 0.9294, 0.8376, 0.8821]]}],\n",
       "                        {},\n",
       "                        {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('ef74dd41-ac8d-4748-b2c3-8d981010bd1e');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence #3\n",
      "English\n",
      "the doctor invited the patient for lunch\n",
      "\n",
      "0. the doctor invitted the patient for lunch\n",
      "\t bert4c similarity: 0.9619\n",
      "\t bert4u similarity: 0.9288\n",
      "\t bert3c similarity: 0.9474\n",
      "\t bert3u similarity: 0.9193\n",
      "\t bert2c similarity: 0.944\n",
      "\t bert2u similarity: 0.9171\n",
      "\t bert1c similarity: 0.9229\n",
      "\t bert1u similarity: 0.8857\n",
      "1. the doctor kartoffeled the patient for lunch\n",
      "\t bert4c similarity: 0.9432\n",
      "\t bert4u similarity: 0.9151\n",
      "\t bert3c similarity: 0.9207\n",
      "\t bert3u similarity: 0.8927\n",
      "\t bert2c similarity: 0.912\n",
      "\t bert2u similarity: 0.884\n",
      "\t bert1c similarity: 0.8813\n",
      "\t bert1u similarity: 0.8494\n",
      "2. Stefan invited the patient for lunch\n",
      "\t bert4c similarity: 0.9436\n",
      "\t bert4u similarity: 0.92\n",
      "\t bert3c similarity: 0.9309\n",
      "\t bert3u similarity: 0.9095\n",
      "\t bert2c similarity: 0.9228\n",
      "\t bert2u similarity: 0.9042\n",
      "\t bert1c similarity: 0.9008\n",
      "\t bert1u similarity: 0.8827\n",
      "3. the doctor invited the patient for sushi\n",
      "\t bert4c similarity: 0.9481\n",
      "\t bert4u similarity: 0.9329\n",
      "\t bert3c similarity: 0.9266\n",
      "\t bert3u similarity: 0.9143\n",
      "\t bert2c similarity: 0.9263\n",
      "\t bert2u similarity: 0.9122\n",
      "\t bert1c similarity: 0.9151\n",
      "\t bert1u similarity: 0.8903\n",
      "4. the doctor invited the patent for lunch\n",
      "\t bert4c similarity: 0.9466\n",
      "\t bert4u similarity: 0.9201\n",
      "\t bert3c similarity: 0.9265\n",
      "\t bert3u similarity: 0.9147\n",
      "\t bert2c similarity: 0.9295\n",
      "\t bert2u similarity: 0.9137\n",
      "\t bert1c similarity: 0.9107\n",
      "\t bert1u similarity: 0.8824\n",
      "5. the doctor invited the patent for linch\n",
      "\t bert4c similarity: 0.8745\n",
      "\t bert4u similarity: 0.8288\n",
      "\t bert3c similarity: 0.8318\n",
      "\t bert3u similarity: 0.8144\n",
      "\t bert2c similarity: 0.8371\n",
      "\t bert2u similarity: 0.813\n",
      "\t bert1c similarity: 0.8029\n",
      "\t bert1u similarity: 0.7535\n",
      "Scores: bert4c: 50%, bert4u: 50%, bert3c: 66%, bert3u: 33%, bert2c: 33%, bert2u: 33%, bert1c: 50%, bert1u: 50%\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "colorscale": "Blues",
         "type": "heatmap",
         "uid": "c8c39623-1271-476a-b009-f02ffdd171a0",
         "y": [
          "bert4c",
          "bert4u",
          "bert3c",
          "bert3u",
          "bert2c",
          "bert2u",
          "bert1c",
          "bert1u"
         ],
         "z": [
          [
           0.9619,
           0.9432,
           0.9436,
           0.9481,
           0.9466,
           0.8745
          ],
          [
           0.9288,
           0.9151,
           0.92,
           0.9329,
           0.9201,
           0.8288
          ],
          [
           0.9474,
           0.9207,
           0.9309,
           0.9266,
           0.9265,
           0.8318
          ],
          [
           0.9193,
           0.8927,
           0.9095,
           0.9143,
           0.9147,
           0.8144
          ],
          [
           0.944,
           0.912,
           0.9228,
           0.9263,
           0.9295,
           0.8371
          ],
          [
           0.9171,
           0.884,
           0.9042,
           0.9122,
           0.9137,
           0.813
          ],
          [
           0.9229,
           0.8813,
           0.9008,
           0.9151,
           0.9107,
           0.8029
          ],
          [
           0.8857,
           0.8494,
           0.8827,
           0.8903,
           0.8824,
           0.7535
          ]
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"a0d6823a-1a12-48bd-9f5d-e0da1be95c73\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    window.PLOTLYENV.BASE_URL='https://plot.ly';\n",
       "                    \n",
       "                if (document.getElementById(\"a0d6823a-1a12-48bd-9f5d-e0da1be95c73\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        'a0d6823a-1a12-48bd-9f5d-e0da1be95c73',\n",
       "                        [{\"colorscale\": \"Blues\", \"type\": \"heatmap\", \"uid\": \"c8c39623-1271-476a-b009-f02ffdd171a0\", \"y\": [\"bert4c\", \"bert4u\", \"bert3c\", \"bert3u\", \"bert2c\", \"bert2u\", \"bert1c\", \"bert1u\"], \"z\": [[0.9619, 0.9432, 0.9436, 0.9481, 0.9466, 0.8745], [0.9288, 0.9151, 0.92, 0.9329, 0.9201, 0.8288], [0.9474, 0.9207, 0.9309, 0.9266, 0.9265, 0.8318], [0.9193, 0.8927, 0.9095, 0.9143, 0.9147, 0.8144], [0.944, 0.912, 0.9228, 0.9263, 0.9295, 0.8371], [0.9171, 0.884, 0.9042, 0.9122, 0.9137, 0.813], [0.9229, 0.8813, 0.9008, 0.9151, 0.9107, 0.8029], [0.8857, 0.8494, 0.8827, 0.8903, 0.8824, 0.7535]]}],\n",
       "                        {},\n",
       "                        {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('a0d6823a-1a12-48bd-9f5d-e0da1be95c73');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#for i in range(min(len(sent_eu), len(sent_en))):\n",
    "for i in range(len(sent_en)):\n",
    "    \n",
    "    print(f\"\\nSentence #{i}\")\n",
    "    print(\"Basque\")\n",
    "    plot_similarities(sent_eu, similarities_all_eu, scores_all_eu)\n",
    "    print(\"English\")\n",
    "    plot_similarities(sent_en, similarities_all_en, scores_all_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculate total scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def calculate_total_score(scores_all):\n",
    "    total_scores = [0] * len(scores_all[0])\n",
    "    for i in range(len(scores_all)):\n",
    "        total_scores[0] += scores_all[i][0]\n",
    "        total_scores[1] += scores_all[i][1]\n",
    "        total_scores[2] += scores_all[i][2]\n",
    "        total_scores[3] += scores_all[i][3]\n",
    "    print(f\"Total scores: \" + \", \".join(f\"{embed}: {scor}\" for embed, scor in zip(embedding_names, total_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total scores: bert8c: 231, bert8u: 264, bert7c: 231, bert7u: 264, bert6c: 0, bert6u: 0, bert6c: 0, bert6u: 0\n"
     ]
    }
   ],
   "source": [
    "#calculate_total_score(scores_all_eu)\n",
    "calculate_total_score(scores_all_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
